<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>9.神经网络 | 木人舟的博客</title><meta name="author" content="木人舟"><meta name="copyright" content="木人舟"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="人工智能文章系列 第1章：AI绪论与概述 第2.1章：知识表示 第2.2章：知识图谱 第3章：确定性推理 第4章：不确定性推理方法 第5章：搜索求解策略 第6章：专家系统 第7章：群智能算法 第8章：机器学习概述 第9章：神经网络  概述本章主要介绍人工神经网络的基本概念，以及几种重要模型，包括“单层感知机、两层感知机、多层感知机”等。 在此基础上，介绍两种重要的基础神经网络“Hopfield神经">
<meta property="og:type" content="article">
<meta property="og:title" content="9.神经网络">
<meta property="og:url" content="https://kashima19960.github.io/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="木人舟的博客">
<meta property="og:description" content="人工智能文章系列 第1章：AI绪论与概述 第2.1章：知识表示 第2.2章：知识图谱 第3章：确定性推理 第4章：不确定性推理方法 第5章：搜索求解策略 第6章：专家系统 第7章：群智能算法 第8章：机器学习概述 第9章：神经网络  概述本章主要介绍人工神经网络的基本概念，以及几种重要模型，包括“单层感知机、两层感知机、多层感知机”等。 在此基础上，介绍两种重要的基础神经网络“Hopfield神经">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/kashima19960/img@master/%E5%85%B6%E4%BB%96/86.jpg">
<meta property="article:published_time" content="2024-07-18T08:00:00.000Z">
<meta property="article:modified_time" content="2024-10-04T13:07:17.888Z">
<meta property="article:author" content="木人舟">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/kashima19960/img@master/%E5%85%B6%E4%BB%96/86.jpg"><link rel="shortcut icon" href="https://cdn.pixabay.com/photo/2024/05/01/03/27/hands-8731277_640.png"><link rel="canonical" href="https://kashima19960.github.io/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 木人舟","link":"链接: ","source":"来源: 木人舟的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '9.神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-04 21:07:17'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/%E5%85%B6%E4%BB%96/86.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电源</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="木人舟的博客"><img class="site-icon" src="https://www.logosc.cn/oss/icons/2022/10/18/Yt1NwjSZdkXXe0s.png"/><span class="site-name">木人舟的博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电源</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">9.神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-18T08:00:00.000Z" title="发表于 2024-07-18 16:00:00">2024-07-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-04T13:07:17.888Z" title="更新于 2024-10-04 21:07:17">2024-10-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="9.神经网络"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="人工智能文章系列"><a href="#人工智能文章系列" class="headerlink" title="人工智能文章系列"></a>人工智能文章系列</h2><ul>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/1.AI%E7%BB%AA%E8%AE%BA%E4%B8%8E%E6%A6%82%E8%BF%B0/" title="1.绪论与概述">第1章：AI绪论与概述</a></li>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/2.1%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/" title="2.1知识表示">第2.1章：知识表示</a></li>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/2.2%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" title="2.2知识图谱">第2.2章：知识图谱</a></li>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/3.%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8E%A8%E7%90%86/" title="3.确定性推理">第3章：确定性推理</a></li>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/4.%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8E%A8%E7%90%86%E6%96%B9%E6%B3%95/" title="4.不确定性推理方法">第4章：不确定性推理方法</a></li>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/5.%E6%90%9C%E7%B4%A2%E6%B1%82%E8%A7%A3%E7%AD%96%E7%95%A5/" title="5搜索求解策略">第5章：搜索求解策略</a></li>
<li><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/6.%E4%B8%93%E5%AE%B6%E7%B3%BB%E7%BB%9F/" title="6专家系统">第6章：专家系统</a></li>
<li><a href="/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/7.%E7%BE%A4%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/" title="7.群智能算法">第7章：群智能算法</a></li>
<li><a href="/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/8.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/" title="8.机器学习概述">第8章：机器学习概述</a></li>
<li><a href="/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" title="9.神经网络">第9章：神经网络</a></li>
</ul>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本章主要介绍人工神经网络的基本概念，以及几种重要模型，包括“单层感知机、两层感知机、多层感知机”等。</p>
<p>在此基础上，介绍两种重要的基础神经网络“Hopfield神经网络、BP神经网络”。</p>
<p>最后，着重介绍了深度学习中最常用的“卷积神经网络”。</p>
<h2 id="人脑结构"><a href="#人脑结构" class="headerlink" title="人脑结构"></a>人脑结构</h2><p>人脑由一千多亿（1011亿－1014 亿）个神经细胞（神经元）交织在一起的网状结构组成，其中大脑皮层约140亿个神经元，小脑皮层约1000亿个神经元。</p>
<h3 id="人脑构造"><a href="#人脑构造" class="headerlink" title="人脑构造"></a>人脑构造</h3><ul>
<li>大脑-皮层（cortex）</li>
<li>中脑（midbrain）</li>
<li>脑干（brainstem）</li>
<li>小脑（cerebellum）</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150106149.png" alt="image-20240718150106149"></p>
<h2 id="人脑神经元结构"><a href="#人脑神经元结构" class="headerlink" title="人脑神经元结构"></a>人脑神经元结构</h2><p>人脑的神经元：约有1000种类型，每个神经元大约与103－104个其他神经元相连接，形成极为错综复杂而又灵活多变的神经网络。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150133616.png" alt="image-20240718150133616"></p>
<p>工作状态：</p>
<p>兴奋状态：细胞膜电位 &gt; 动作电位的阈值 → 神经冲动</p>
<p>抑制状态：细胞膜电位 &lt; 动作电位的阈值</p>
<p>学习与遗忘：由于神经元结构的可塑性，突触的传递作用可增强和减弱 。</p>
<h2 id="人脑神经元结构-–-术语"><a href="#人脑神经元结构-–-术语" class="headerlink" title="人脑神经元结构 – 术语"></a>人脑神经元结构 – 术语</h2><p>人脑神经网络是一个具有学习能力的系统，不同神经元之间的突触有强有弱，其强度是可以通过学习（训练）不断改变，具有一定可塑性。单个神经元的神经活动不具备重要性，关键是神经元之间如何组成一个复杂的网络。</p>
<table>
<thead>
<tr>
<th>神经元结构</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>细胞体  (Soma)</td>
<td>包含细胞核和其他细胞器，是神经元的代谢中心。</td>
</tr>
<tr>
<td>细胞膜</td>
<td>含有各种受体和离子通道，是神经元兴奋和抑制的产生部位。</td>
</tr>
<tr>
<td>树突  (Dendrite)</td>
<td>接收来自其他神经元的信号，并将兴奋传入细胞体。</td>
</tr>
<tr>
<td>轴突(Axon)</td>
<td>将自身的兴奋状态从胞体传送到另一个神经元或其他组织。</td>
</tr>
<tr>
<td>突触  (Synapse)</td>
<td>神经元之间的连接“接口”，将一个神经元的兴奋状态传到另一个神经元。</td>
</tr>
</tbody></table>
<h2 id="人工神经网络-起源与概念"><a href="#人工神经网络-起源与概念" class="headerlink" title="人工神经网络 - 起源与概念"></a>人工神经网络 - 起源与概念</h2><p>生物神经网络( natural neural network, NNN): 由中枢神经系统（脑和脊髓）及周围神经系统（感觉神经、运动神经等）所构成的错综复杂的神经网络，其中最重要的是脑神经系统。</p>
<p>人工神经网络(artificial neural networks, ANN): 模拟人脑神经系统的结构和功能，运用大量简单处理单元经广泛连接而组成的人工网络系统。</p>
<h2 id="神经网络发展演进-三起三落"><a href="#神经网络发展演进-三起三落" class="headerlink" title="神经网络发展演进 - 三起三落"></a>神经网络发展演进 - 三起三落</h2><p>神经网络发展曲折，从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150214422.png" alt="image-20240718150214422"></p>
<p>这两个10年间人们对于神经网络的期待并不现在低，可结果都逐渐衰落。</p>
<p>冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有 “钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150226540.png" alt="image-20240718150226540"></p>
<h2 id="神经网络的学习-决定性能的三大要素"><a href="#神经网络的学习-决定性能的三大要素" class="headerlink" title="神经网络的学习 - 决定性能的三大要素"></a>神经网络的学习 - 决定性能的三大要素</h2><p>神经网络方法：是一种“知识表示方法和推理方法”。</p>
<p>神经网络知识表示：是一种隐式的表示方法，它将某个问题的若干知识通过学习表示在一个网络中。（谓词、产生式、语义网络等是显示表示法）</p>
<p>神经网络的学习：指调整神经网络的“连接权值或结构”，使输入和输出能满足需要。</p>
<h3 id="决定人工神经网络性能的三大要素"><a href="#决定人工神经网络性能的三大要素" class="headerlink" title="决定人工神经网络性能的三大要素"></a>决定人工神经网络性能的三大要素</h3><ul>
<li>神经元的特性。</li>
<li>神经元之间相互连接的形式 – 拓扑结构（见下页PPT）。</li>
<li>为适应环境而改善性能的学习规则。</li>
</ul>
<h3 id="Hebb学习规则"><a href="#Hebb学习规则" class="headerlink" title="Hebb学习规则"></a>Hebb学习规则</h3><p>1944年，赫布提出改变神经元连接强度的规则：学习过程最终发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。当某一突触两端的神经元同时处于兴奋状态，那么该连接的权值应该增强。（教材P215给出了数学表达公式）</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150242624.png" alt="image-20240718150242624"></p>
<h2 id="神经网络结构-分类"><a href="#神经网络结构-分类" class="headerlink" title="神经网络结构 - 分类"></a>神经网络结构 - 分类</h2><p>根据神经网络中神经元的连接方式，可以划分为不同类型。主要包括“前馈型、反馈型”两种。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150258207.png" alt="image-20240718150258207"></p>
<h3 id="前馈型（-前向型）"><a href="#前馈型（-前向型）" class="headerlink" title="前馈型（ 前向型）"></a>前馈型（ 前向型）</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150301748.png" alt="image-20240718150301748"></p>
<h3 id="反馈型"><a href="#反馈型" class="headerlink" title="反馈型"></a>反馈型</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150316109.png" alt="image-20240718150316109"></p>
<h2 id="神经网络结构-应用"><a href="#神经网络结构-应用" class="headerlink" title="神经网络结构 - 应用"></a>神经网络结构 - 应用</h2><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150357971.png" alt="image-20240718150357971"></p>
<h2 id="神经元-MP模型（单层感知机）"><a href="#神经元-MP模型（单层感知机）" class="headerlink" title="神经元 - MP模型（单层感知机）"></a>神经元 - MP模型（单层感知机）</h2><p>AI诞生之时，很容易联想到是否可借鉴人脑的构成。尽管人脑的奥秘还存在很多未知领域，但在已知领域，科学家一直在尝试让计算机模拟人脑运行。首先要做的，就是从人脑的最小单元—神经元入手，让计算机模拟它的工作机制。</p>
<p>1943年，麦克洛奇和皮兹发现了大脑中神经元的工作机制MP模型。</p>
<h3 id="真实的人脑神经3D模拟"><a href="#真实的人脑神经3D模拟" class="headerlink" title="真实的人脑神经3D模拟"></a>真实的人脑神经3D模拟</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150416064.png" alt="image-20240718150416064"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150422241.png" alt="image-20240718150422241"></p>
<h2 id="单层感知机-数学模型"><a href="#单层感知机-数学模型" class="headerlink" title="单层感知机 - 数学模型"></a>单层感知机 - 数学模型</h2><p>图中每个中枢点可认为是一个神经元，把神经元的生物工作机制简单的绘制出来，并以此建模，得到一个计算机能识别的模型，称之为—Perceptron（感知器）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150433990.png" alt="image-20240718150433990"></p>
<blockquote>
<p>可以把一个神经元细胞想象成一个有多个听筒（输入），但只有一个话筒（输出）的电话。</p>
<p>听筒就是神经元的树突，话筒是神经元的轴突。</p>
</blockquote>
<h2 id="单层感知机-激活函数（输出变换函数）"><a href="#单层感知机-激活函数（输出变换函数）" class="headerlink" title="单层感知机 - 激活函数（输出变换函数）"></a>单层感知机 - 激活函数（输出变换函数）</h2><p>激活函数：也叫非线性激励函数、输出变换函数。</p>
<p>激活的概念意味着：通过某个门槛值就是1，否则是0。不恰当的比喻是考试到60分就及格，可以升级。从这个定义来看考0和59是一样的不会被激活，而60和100是一样的，都会被激活。</p>
<h2 id="单层感知机的本质-线性分类器"><a href="#单层感知机的本质-线性分类器" class="headerlink" title="单层感知机的本质 - 线性分类器"></a>单层感知机的本质 - 线性分类器</h2><p>感知器中的权值是通过训练得到的。因此，根据以前机器学习的知识可知，感知器类似一个逻辑回归模型，可以做线性分类任务。可以用决策分界来形象的表达分类的效果。</p>
<p>决策分界：就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。</p>
<p>感知机可以被视为一种最简单形式的前馈式人工神经网络，它是一种二分类的线性分类判别模型， 其输入为实例的特征向量想（x1,x2…），神经元的激活函数f为sign，输出为实例的类别（+1或者-1），模型的目标是要将输入实例通过超平面将正负二类分离。</p>
<p>下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150510264.png" alt="image-20240718150510264"></p>
<h2 id="单层感知机-局限性（无法计算异或XOR）"><a href="#单层感知机-局限性（无法计算异或XOR）" class="headerlink" title="单层感知机 - 局限性（无法计算异或XOR）"></a>单层感知机 - 局限性（无法计算异或XOR）</h2><p>把异或运算的两个输入当做感知器的输入值，期待感知器能把输入值加权求和，然后再用一个激活函数得到两个输入值的异或值。同样我们像刚才一样将A,B 的四个值（0,0), (0,1),(1,0),(1,1)作为X1, X2画在坐标系里（更准确的此时应该说是一个二维向量空间）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150547264.png" alt="image-20240718150547264"></p>
<p>请问是否可以找到一条线将红绿点分开？答案是不能。似乎感知器真不能解决这样简单的问题。</p>
<p>那为什么我们现在还在用这个模型呢？ 答案是MLP(Multilayer perceptron)，多层感知器。</p>
<h2 id="两层感知机-明斯基的“功与罪”"><a href="#两层感知机-明斯基的“功与罪”" class="headerlink" title="两层感知机 - 明斯基的“功与罪”"></a>两层感知机 - 明斯基的“功与罪”</h2><p>1951年在普林斯顿攻读phD时，年仅24岁的Minsky发明了第一台物理的基于感知器的人工神经网络- 随机神经模拟强化计算器SNARC （ Stochastic Neural Analog Reinforcement Calculator）。在Minsky晚年接受采访时，仍然不忘拿出来show了一把。按理说他应该是感知器的发扬光大者才对。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150602294.png" alt="image-20240718150602294"></p>
<p>但是他非常聪明，很早就发现了“单层感知器”的局限，这也体现在1969年他和 Parpet 写 的 一 本 书 中 ， 书 的 名 字 叫 《Perceptrons: An Introduction to Computational Geometry》。书中提到最为典型的例子是感知器无法解决像“异或”这么简单的问题。</p>
<h2 id="两层感知机-罗森布拉特（0到1的突破）"><a href="#两层感知机-罗森布拉特（0到1的突破）" class="headerlink" title="两层感知机 - 罗森布拉特（0到1的突破）"></a>两层感知机 - 罗森布拉特（0到1的突破）</h2><p>Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。算力受限：不过两层神经网络的计算是一个问题，在当时算力受限的情况下，并没有一个较好的解法。1961年，Frank Rosenblatt（感知器模型的坚定支持者，也是明斯基的高中同学）就发表了论文《Perceptrons and the theory of brain mechanisms》，提出多层感 知器的概念，只是当时并没有被人注意到。非常遗憾，Rosenblatt在明斯基出版《Perceptrons: An Introduction to Computational Geometry》一书的同一年（1969年），就英年早逝（享年41岁）。甚至有人说，如果Rosenblatt在世，可能Marvin根本不会是图灵奖得主。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150617282.png" alt="image-20240718150617282"></p>
<h2 id="两层感知器-解决异或XOR问题"><a href="#两层感知器-解决异或XOR问题" class="headerlink" title="两层感知器 - 解决异或XOR问题"></a>两层感知器 - 解决异或XOR问题</h2><p>异或运算举例，假使我们不是做一条线，而是做两条线是不是就可以将红绿点分开了呢。</p>
<p>拆解一下这两条线划分的步骤，以及它如何解决了异或问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150637811.png" alt="image-20240718150637811"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150632502.png" alt="image-20240718150632502"></p>
<h2 id="两层感知器-解决异或XOR问题-1"><a href="#两层感知器-解决异或XOR问题-1" class="headerlink" title="两层感知器 - 解决异或XOR问题"></a>两层感知器 - 解决异或XOR问题</h2><p>第1步：我们在单层感知器的中间加上一层隐式层，在隐式层里加上两个神经元h1， h2。</p>
<p>第2步：X1, X2信号经过h1感知器后，可以将点(1,0)和其他点分开。</p>
<p>第3步：X1, X2信号经过h2感知器后，可以将点(0,1)和其他点分开，在这里，我们对h2的激活函数</p>
<p>第4步：此时，h1，h2就会有三种输出，即（红，绿），（绿，绿），（绿，红），我们用0代表红色，1代表绿色，那就是(0,1), (1,1), (1,0) 就很容易分开了。这样我们就用一个多层感知器解决了异或XOR问题。</p>
<p>用通用的方法表示以上结构就是下图的神经网络的雏形：</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150707437.png" alt="image-20240718150707437"></p>
<h2 id="两层感知器的本质-非线性分类器"><a href="#两层感知器的本质-非线性分类器" class="headerlink" title="两层感知器的本质 - 非线性分类器"></a>两层感知器的本质 - 非线性分类器</h2><p>从以上可看出，感知器本质上就是一个分类器，计算机首先通过学习来划出一个空间（多维向 量空间），然后根据这个学习结果来区分新的 输入是落在这个空间之外，还是之内。计算机不可能像人通过“看一眼”就能划出这个空间，它是采用的是无限逼近的方法。（比如随机给一组权重，然后看看通过这个权重算出来的值与训练给出的已知值有多大差异，直到这个差异值达到最小值，就停止逼近。）如何使计算机更快更好的逼近这个最佳权重，就是所有的基于神经网络的算法要解决的问题。（最简单的比如刚做的将单层感知器增加一层，还比如增加神经元，都可以更好更快地划出这个空间。）【右图是斯坦福网站<a href="https://link.zhihu.com/?target=https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">有趣的实验</a>，支持2层神经网络】当把一层感知器增加为两层的时候，就会画出两条线来来划出一个二维空间。在两层感知器中，通过增加神经元可以对更复杂的点分布做出分类。即，可通过多层神经元这种结构可以对一切进行分类。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150724105.png" alt="image-20240718150724105"></p>
<h2 id="多层感知机-深度学习（1到100的发展）"><a href="#多层感知机-深度学习（1到100的发展）" class="headerlink" title="多层感知机 - 深度学习（1到100的发展）"></a>多层感知机 - 深度学习（1到100的发展）</h2><p>多层感知机通过对线性分类器的组合叠加，具有拟合非线性函数的能力。Hornik在1989年证明，当中间隐含层的神经元数量趋于无穷多时，多层感知机可以拟合任何非线性函数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150737296.png" alt="image-20240718150737296"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150740253.png" alt="image-20240718150740253"></p>
<h2 id="神经网络发展小结-算法、算力的影响"><a href="#神经网络发展小结-算法、算力的影响" class="headerlink" title="神经网络发展小结 - 算法、算力的影响"></a>神经网络发展小结 - 算法、算力的影响</h2><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150753227.png" alt="image-20240718150753227"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150756259.png" alt="image-20240718150756259"></p>
<h2 id="神经网络发展小结-扩展"><a href="#神经网络发展小结-扩展" class="headerlink" title="神经网络发展小结 - 扩展"></a>神经网络发展小结 - 扩展</h2><p>理论上来说，神经元数量越多，层数越多，产生的力量越大。可借鉴互联网的网络效应，网络效应粗略可根据节点间的连接数量来确定，而网络的力量就蕴含在这些连接当中。（假设一个网络中有N的节点，最佳情况下，每两个节点都可以相连，就可以产生： N（N-1）&#x2F;2次连接（1个节点可以和其他N-1个节点连接，总共就是N（N-1）个连接，但因为每个连接会重复算一次，所以要除以2））。在人类大脑里，平均每人拥有860亿个神经元，这860个神经元总共有100万亿个突触，即可以发生100万亿次连接（这里并非每两个神经元之间都有连接)。在已知领域内，大脑的所产生的力量也是蕴含在这些神经元的连接中（至于其他未知领域，相信还有很多，毕竟人脑进化了几百万年，不是我们几百年就能研究清楚的），所以你大概能知道人与人脑力的差别可能就在于：神经元的数量。神经元之间的连接，这种连接是可以被训练的。20世纪60年前关于感知器局限性的争论，然后又对AI这座大厦最底层原理做了一个简单的阐述。最大感受是：一座大厦，不管它多么宏伟，也离不开底层的一砖一瓦；一个个体，不管它多么渺小，汇聚在一起也能产生意想不到的力量。在整个人工智能的知识体系里，感知器这个小小的概念最终扛住了时间的考验，成为今天的AI的基石。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150811982.png" alt="image-20240718150811982"></p>
<h2 id="Hopfield网络-概念与原理"><a href="#Hopfield网络-概念与原理" class="headerlink" title="Hopfield网络 - 概念与原理"></a>Hopfield网络 - 概念与原理</h2><p>1982年， Hopfield的一篇论文横空出世，仿佛神经网络这匹困兽再次苏醒，也预示着神经网络在AI 领域的崛起。论文中提到的associative neural network就是后来的“Hopfield network”，它是当前学习神经网络绕不开的话题。Hopfield 并非图灵奖得主，但他对计算机AI 领域的开创性贡献却启发了三位未来的图灵奖得主（ Geoffery Hiton, Bengjo, Yann Lecun）。Hopfield 所从事的领域并非计算机科学，Hopfield在物理学的的成就更为显著。在2001年， Hopfield 被授予Dirac奖（理论物理学界的诺贝尔奖）Hopfield 网络就是他从物理学角度去研究神经科学，最后却被应用于计算机科学的一个案例。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150833703.png" alt="image-20240718150833703"></p>
<h2 id="Hopfield网络-伊辛模型（Ising-model）"><a href="#Hopfield网络-伊辛模型（Ising-model）" class="headerlink" title="Hopfield网络 - 伊辛模型（Ising model）"></a>Hopfield网络 - 伊辛模型（Ising model）</h2><p>伊辛模型：在解释物体呈现“固态、液态或者气态”，是内部粒子实现“动态平衡”的表现。就像湖里的水不变，不是因为它没有输入输出，只是因为输入输出恰好一样。类似，Hopefield网络认为，人脑记忆也是一种动态平衡。当磁体在高温时磁性会消失，而降到一定温度后，又会恢复磁性。磁性的产生是因为磁体内部粒子的磁性方向一致。高温时，粒子的磁性方向杂乱无章，彼此磁性抵消，呈现整体没有磁性的状态。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150859903.png" alt="image-20240718150859903"></p>
<p>假设某个内容被记忆（存储）在N个神经元中。和伊辛模型类似，Hopfield认为每个神经元的状态仅仅与它相邻神经元的状态相关，同时反过来，每个神经元的状态又影响着与其相邻的神经元。</p>
<p>这种动态平衡是由这样一个规则在维持：当相邻两个神经元的状态相反时（相当于彼此想让对方翻转），彼此作为输入信号的权重为-1， 当相邻两个粒子状态相同时，权重为+1。</p>
<h2 id="Hopfield神经网络-网络结构"><a href="#Hopfield神经网络-网络结构" class="headerlink" title="Hopfield神经网络 - 网络结构"></a>Hopfield神经网络 - 网络结构</h2><p>Hopfield神经网络是反馈神经网络，其输出端又会反馈到其输入端，在输入的激励下，其输出会产生不断的状态变化，这个反馈过程会一直反复进行。假如Hopfield神经网络是一个收敛的稳定网络，则这个反馈与迭代的计算过程所产生的变化越来越小，一旦达到了稳定的平衡状态，Hopfield网络就会输出一个稳定的恒值（吸引因子(W矩阵)）。对于一个Hopfield神经网络来说，关键在于确定它在稳定条件下的权重系数。原始Hopfield神经网络是个全连接网络，即网络中任意两个神经元之间都有连接，在数学上这叫完全图（complete graph）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150918897.png" alt="image-20240718150918897"></p>
<p>可以认为Hopfield网络里的神经元都是社交高手，跟谁都是朋友。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150925072.png" alt="image-20240718150925072"></p>
<h2 id="Hopfield神经网络-平衡状态规则"><a href="#Hopfield神经网络-平衡状态规则" class="headerlink" title="Hopfield神经网络 - 平衡状态规则"></a>Hopfield神经网络 - 平衡状态规则</h2><p>动态平衡的维持规则：当相邻两个神经元的状态相反时（相当于彼此想让对方翻转），彼此作为输入信号的权重为-1， 当相邻两个粒子状态相同时，权重为+1</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718150955364.png" alt="image-20240718150955364"></p>
<h3 id="权重计算"><a href="#权重计算" class="headerlink" title="权重计算"></a>权重计算</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718151007450.png" alt="image-20240718151007450"></p>
<h3 id="结果计算"><a href="#结果计算" class="headerlink" title="结果计算"></a>结果计算</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718151017674.png" alt="image-20240718151017674"></p>
<h2 id="Hopfield神经网络-求解权重矩阵"><a href="#Hopfield神经网络-求解权重矩阵" class="headerlink" title="Hopfield神经网络 - 求解权重矩阵"></a>Hopfield神经网络 - 求解权重矩阵</h2><p>以此类推，经过这样多个权重定义，其他几个神经元的状态也能保持状态不变。因此，核心任务转换为“求解权重矩阵”。特点： 权重矩阵W 完全无需人工参与调节， 由神经元自身状态值产生， 因此，Hopefield 网络也被认为是第一个无监督式学习模型。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718151033682.png" alt="image-20240718151033682"></p>
<h2 id="Hopfield网络-如何通过片段回忆整个记忆内容"><a href="#Hopfield网络-如何通过片段回忆整个记忆内容" class="headerlink" title="Hopfield网络 - 如何通过片段回忆整个记忆内容"></a>Hopfield网络 - 如何通过片段回忆整个记忆内容</h2><p>要想通过片段唤起整个记忆，相当于是把片段作为一个输入值，经过某种变换之后，输出整个记忆。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152444135.png" alt="image-20240718152444135"></p>
<p>当然，这只是4个神经元的恢复，现实中可能是成千上万个神经元参与，这个回忆过程不会一步完成，而是一个围绕记忆上下振荡的过程，但是由于W矩阵的存在，最终会恢复到记忆，就像我们回忆某个事情的时候，也要经历一些过程。</p>
<h2 id="BP神经网络-概念"><a href="#BP神经网络-概念" class="headerlink" title="BP神经网络 - 概念"></a>BP神经网络 - 概念</h2><ul>
<li>定义：是一种按误差逆传播算法训练的多层前馈神经网络，是目前应用很广泛。</li>
<li>作用：BP网络能学习和存贮大量的“输入-输出”模式映射关系，而无需事前揭示描述这种映射关系的数学方程。</li>
<li>学习规则：使用“最速梯度下降法”，通过反向传播来“不断调整网络的权值和阈值”，使网络的误差平方和最小。</li>
<li>基本思想：通过计算“输出层”与“期望值”之间的误差来调整网络参数，从而使得误差变小。其思想很简单，然而人们认识到它的重要作 用却经过了很长的时间。</li>
<li>重要性：BP算法是深度学习中求取各层梯度的核心算法，理解该算法对于理解深度学 习原理至关重要。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152503535.png" alt="image-20240718152503535"></p>
<h2 id="BP神经网络-起源"><a href="#BP神经网络-起源" class="headerlink" title="BP神经网络 - 起源"></a>BP神经网络 - 起源</h2><p>1974年，哈佛大学沃伯斯(Paul Werbos)博士论文里，首次提出了通过误差的反向传播(BP，Backpropagation)来训练人工神经网络，但在该时期未引起重视。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152516826.png" alt="image-20240718152516826"></p>
<p>1986 年， Rumelhart 和Hinton 等发展了反向传播BP算法，解决了两层神经网络所需要的复杂计算量问题，带动业界使用两层神经网络研究的热潮。（ 参 见 他 们 发 表 在 Nature 上 的 论 文 Learning  representations  by  back- propagating errors ）</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152531739.png" alt="image-20240718152531739"></p>
<h2 id="BP神经网络-算法原理"><a href="#BP神经网络-算法原理" class="headerlink" title="BP神经网络 - 算法原理"></a>BP神经网络 - 算法原理</h2><ul>
<li>MLP存在问题：多层感知器在如何获取“隐层的权值”问题上遇到了瓶颈。</li>
<li>解决思路：既然我们无法直接得到隐层的权值，能否先通过输出层得到“输出结果和期</li>
<li>望输出的误差”来间接调整隐层的权值呢？ BP算法就是采用这样的思想设计出来的。</li>
<li>基本思想：学习过程由“信号的正向传播”与“误差的反向传播”两个过程组成。。</li>
<li>前向传播（阶段1）：从输入层开始，逐层计算输出，直至输出层。“每个神经元的输出”通过加权和和激活函数的处理得到。</li>
<li>反向传播：在前向传播结束后，通过比较“网络输出”和“期望输出”的差异来计算误差。然后，误差以反向传播的方式逐层传递回输入层，通过调整各层间连接权重，使误差逐步减小。</li>
<li>权值更新：在反向传播过程中，根据误差和梯度下降法，更新神经网络的权值和阈值。通过不断迭代，使得网络输出逼近期望输出，达到训练的目标。</li>
</ul>
<h2 id="BP神经网络-算法原理（通俗举例解释）"><a href="#BP神经网络-算法原理（通俗举例解释）" class="headerlink" title="BP神经网络 - 算法原理（通俗举例解释）"></a>BP神经网络 - 算法原理（通俗举例解释）</h2><ul>
<li>前向传播：三个人在玩你画我猜的游戏，然后第一个人给第二个人描述，再将信息传递给第三个人，由第三个人说出画的到底是啥。</li>
<li>反向传播：第三个人得知自己说的和真实答案之间的误差后，发现他们在传递时的问题差在哪里，向前面一个人说下次描述的时候怎样可以更加准确的传递信息。就这样一直向前一个人告知。</li>
</ul>
<p>不断磨合：三个人之间的的默契一直在磨合，然后描述的更加准确。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152551154.png" alt="image-20240718152551154"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152555467.png" alt="image-20240718152555467"></p>
<h2 id="BP神经网络-数学模型"><a href="#BP神经网络-数学模型" class="headerlink" title="BP神经网络 - 数学模型"></a>BP神经网络 - 数学模型</h2><p>BP网络结构：在输入层与输出层之间增加若干层(一层或多层)神经元，这些神经元称为隐藏层，它们与外界没有直接的联系，但其状态的改变，则能影响输入与输出之间的关系，每一层可以有若干个节点。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152827898.png" alt="image-20240718152827898"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152830546.png" alt="image-20240718152830546"></p>
<p>要解决的问题：如何获取“隐层的权值(W, b) ” 。</p>
<h2 id="BP神经网络-前向传播"><a href="#BP神经网络-前向传播" class="headerlink" title="BP神经网络 - 前向传播"></a>BP神经网络 - 前向传播</h2><p>过程：从输入层开始，逐层计算输出，直至输出层。每个神经元的输出通过加权和和激活函数的处理得到。</p>
<p>即：构建函数模型，并计算出Loss函数 的过程。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152848700.png" alt="image-20240718152848700"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152859811.png" alt="image-20240718152859811"></p>
<h2 id="BP神经网络-反向传播（反馈与调整）"><a href="#BP神经网络-反向传播（反馈与调整）" class="headerlink" title="BP神经网络 - 反向传播（反馈与调整）"></a>BP神经网络 - 反向传播（反馈与调整）</h2><p>过程：在已知Loss函数 的情况下，对该函数做数学优化，得到最小化Loss函数 时，各个参数(W, b) 的值。</p>
<p>优化技巧（梯度下降法）：利用Loss函数 求得其关于所有参数(W, b) 的梯度，再基于梯度下降法更新参数。</p>
<p>反向传播知道如何更改网络中的权重w和偏差b，来改变代价函数Loss函数值。最终这意味着它能够计算关于(W, b) 的偏导数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152921285.png" alt="image-20240718152921285"></p>
<p>为计算以上w和b的偏导数，先引入一个中间变量（网络中第 𝑙 层第 𝑗 个神经元的误差）。后向传播能够计算出误差，即可返回计算出w和b。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152940319.png" alt="image-20240718152940319"></p>
<p>上式表示“损失函数在 𝑧 [𝑙] 上的偏导数”，为什么误差能定义成这样？这是因为目标点在损失函数曲面的最低点的梯度为0，即所有维度上的偏导值为0，所以将当前点处的偏导值定义为当前的误差是非常合理的。</p>
<p>根据链式法则</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718152958476.png" alt="image-20240718152958476"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153029627.png" alt="image-20240718153029627"></p>
<p>以下表示本层δ值 与下一层δ值 间的关系，这种反向计算关系，就是反向传播算法这个名字的由来。其实误差并没有真的在神经网络中传播，而是上一层的δ值与下一层δ值之间存在这种数学关系，所谓误差的反向传播就是对这种反向计算方式的形象表达。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153056193.png" alt="image-20240718153056193"></p>
<p>注意：截图中的的C函数就是Loss函数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153150551.png" alt="image-20240718153150551"></p>
<h2 id="BP神经网络-小结"><a href="#BP神经网络-小结" class="headerlink" title="BP神经网络 - 小结"></a>BP神经网络 - 小结</h2><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153157791.png" alt="image-20240718153157791"></p>
<h2 id="卷积神经网络CNN-起源与原理"><a href="#卷积神经网络CNN-起源与原理" class="headerlink" title="卷积神经网络CNN - 起源与原理"></a>卷积神经网络CNN - 起源与原理</h2><p>1968年，Hubel和Wiesel的论文，讲述猫和猴的视觉皮层含有对视野的小区域单独反应的神经元。</p>
<p>感受野（Receptive Field）：如果眼睛没有移动，则视觉刺激影响单个神经元的视觉空间区域。相邻细胞具有相似和重叠的感受野。感受野大小和位置在皮层之间系统地变化，形成完整的视觉空间图。</p>
<p>以上为CNN的局部感知奠定了一个基础。</p>
<p>Hubel和Wiesel将猫麻醉后，把电极插到其视觉神经上，并连接示波器。然后给它们看不同图像，观察脑电波反应。发现猫看到鱼的图片神经元并不会兴奋。但意外发现，切换幻灯片时，猫神经元会兴奋，即“图片的边缘会引起猫咪神经元的兴奋”。</p>
<p>由此，两人获得1981年诺贝尔奖。这项发现在生物学上留下浓墨重彩的一笔，且对20年后人工智能的发展埋下了伏笔。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153722276.png" alt="image-20240718153722276"></p>
<blockquote>
<p>1980年，日本人福岛邦彦在论文《Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffectedby shift in position》提出一个包含卷积层、池化层的神经网络结构。</p>
</blockquote>
<h2 id="卷积神经网络CNN-突破-LeNet-5"><a href="#卷积神经网络CNN-突破-LeNet-5" class="headerlink" title="卷积神经网络CNN - 突破(LeNet-5)"></a>卷积神经网络CNN - 突破(LeNet-5)</h2><p>1998年，在这个基础上，Yann Lecun在论文《Gradient-Based Learning Applied to Document Recognition》中提出了LeNet-5，将BP算法应用到这个神经网络结构的训练上，就形成了当代卷积神经网络的维形。</p>
<p>Yann Lecun最早将CNN用于手写数字识别。</p>
<p>LeNet-5的结构和现在用的 CNN网络结构已经非常接近。网络层数加深到了7层，其中两层卷积两层池化。</p>
<p>LeNet-5 标志卷积神经网络的开端，因为当时计算机算力的限制， 所以其使用了复杂的局部连接。</p>
<p>并 且 当 时 并 没 有 使 用 softmax和交叉熵，而是使用了欧式径向基函数和均方误差。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153739809.png" alt="image-20240718153739809"></p>
<h2 id="卷积神经网络-结构"><a href="#卷积神经网络-结构" class="headerlink" title="卷积神经网络 - 结构"></a>卷积神经网络 - 结构</h2><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153749182.png" alt="image-20240718153749182"></p>
<table>
<thead>
<tr>
<th>名称</th>
<th>操作内容</th>
<th>数学意义</th>
</tr>
</thead>
<tbody><tr>
<td>输入层</td>
<td>接收原始图像数据（常是像素矩阵）；  对彩色图像，矩阵的深度（即通道数）通常为3，代表红、绿、蓝。</td>
<td>-</td>
</tr>
<tr>
<td>卷积层  （核心）</td>
<td>拿着滤镜查找和提取特征；  滤波器在输入数据上滑动并进行卷积运算，以提取输入数据的局部特征。  随着网络深度的增加，卷积层能够从低级特征中提取更复杂的特征。</td>
<td>卷积就是2个函数的叠加，即矩阵相乘。</td>
</tr>
<tr>
<td>池化层  （下采样层）</td>
<td>为减少训练的参数，在保持采样不变下，忽略掉一些信息，降低网络的参数数量。  同时增强对微小形变的鲁棒性。</td>
<td>取最大值、或平均值等，达到降维目的。</td>
</tr>
<tr>
<td>全连接层</td>
<td>将卷积层和池化层提取的特征进行整合  做分类或回归任务判断，并输出最终预测结果。</td>
<td>矩阵相乘</td>
</tr>
</tbody></table>
<h2 id="CNN算法框架1-卷积层（卷积操作）"><a href="#CNN算法框架1-卷积层（卷积操作）" class="headerlink" title="CNN算法框架1 - 卷积层（卷积操作）"></a>CNN算法框架1 - 卷积层（卷积操作）</h2><p>主要是用来提取图像数据的特征和学习图像数据的特征表示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153808784.png" alt="image-20240718153808784"></p>
<h2 id="CNN算法框架-卷积层（子卷积层）"><a href="#CNN算法框架-卷积层（子卷积层）" class="headerlink" title="CNN算法框架 - 卷积层（子卷积层）"></a>CNN算法框架 - 卷积层（子卷积层）</h2><p>输入图像有RGB 3个通道，会增加网络的深度，此时就有了子卷积层的概念。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153820083.png" alt="image-20240718153820083"></p>
<h2 id="CNN算法框架-卷积层"><a href="#CNN算法框架-卷积层" class="headerlink" title="CNN算法框架 - 卷积层"></a>CNN算法框架 - 卷积层</h2><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153843197.png" alt="image-20240718153843197"></p>
<h3 id="选择卷积的步长"><a href="#选择卷积的步长" class="headerlink" title="选择卷积的步长"></a>选择卷积的步长</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153930566.png" alt="image-20240718153930566"></p>
<h3 id="卷积核个数的确定"><a href="#卷积核个数的确定" class="headerlink" title="卷积核个数的确定"></a>卷积核个数的确定</h3><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153942232.png" alt="image-20240718153942232"></p>
<h2 id="CNN算法框架-池化层（下采样）"><a href="#CNN算法框架-池化层（下采样）" class="headerlink" title="CNN算法框架 - 池化层（下采样）"></a>CNN算法框架 - 池化层（下采样）</h2><p>作用：是减小数据处理量同时保留有用信息。理论上，任何类型的操作（如求最大、求平均等）都可以在池化层中完成，但实际上，一般只使用最大池化，这是因为卷积已经提取出特征，相邻区域的特征是类似，近乎不变。这时，池化只是选出最能表征特征的像素，缩减了数据量，同时保留了特征。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153953415.png" alt="image-20240718153953415"></p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718153956350.png" alt="image-20240718153956350"></p>
<h2 id="CNN算法框架-池化层（技术分析）"><a href="#CNN算法框架-池化层（技术分析）" class="headerlink" title="CNN算法框架 - 池化层（技术分析）"></a>CNN算法框架 - 池化层（技术分析）</h2><p>池化是一种down-sampling技术，本质是基于滑动窗口的思想，可以去除特征图中的冗余信息，对卷积层结果压缩的到重要特征，同时还可以有效的控制过拟合。池化一般通过简单的“最大值、最小值或平均值”操作完成。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/84c61769fc59414897356d02330c39b9.png" alt="在这里插入图片描述"></p>
<h2 id="CNN算法框架-激活函数"><a href="#CNN算法框架-激活函数" class="headerlink" title="CNN算法框架 - 激活函数"></a>CNN算法框架 - 激活函数</h2><p>CNN最成功的非线性是整流非线性单元（ReLU），它可以解决sigmoids中出现的消失梯度问题。</p>
<p>缺点：ReLU更容易计算并产生稀疏性（因此，并不总是有益）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/260f259d860f4a5281ffe9e5b8726db5.png" alt="在这里插入图片描述"></p>
<h2 id="CNN算法框架-分批归一化（Batch-Nomarliaztion）"><a href="#CNN算法框架-分批归一化（Batch-Nomarliaztion）" class="headerlink" title="CNN算法框架 - 分批归一化（Batch Nomarliaztion）"></a>CNN算法框架 - 分批归一化（Batch Nomarliaztion）</h2><p>为了解决层与层之间传输数据，前面层的数据更新，导致后面层数据分布的变化，会产生网络收敛慢、学习速度降低问题。</p>
<p>分批归一化：简化了计算过程，并保留了原始数据的表达能力。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/6ec1420b23b6469d85fb8f96ab35d5e3.png" alt="在这里插入图片描述"></p>
<h2 id="CNN算法框架-全连接层-dense-layer"><a href="#CNN算法框架-全连接层-dense-layer" class="headerlink" title="CNN算法框架 - 全连接层 (dense layer)"></a>CNN算法框架 - 全连接层 (dense layer)</h2><p>定义：当一层中的神经元与前一层的所有神经元都相连时，该层则称为全连接层。</p>
<p>主要作用：是将输入图像，在经过卷积和池化操作后提取的特征进行压缩，并且根据压缩的特征，完成模型的分类功能。</p>
<p>对经过多次卷积层和多次池化层所得出来的高级特征进行全连接（全连接就是常规神</p>
<p>经网络的性质），算出最后的预测值。</p>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/888773ca8b244d3389c9f34687a38d7f.png" alt="在这里插入图片描述"></p>
<h2 id="卷积神经网络CNN-特点总结"><a href="#卷积神经网络CNN-特点总结" class="headerlink" title="卷积神经网络CNN - 特点总结"></a>卷积神经网络CNN - 特点总结</h2><table>
<thead>
<tr>
<th></th>
<th>类别</th>
<th>内容</th>
<th>优点</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>局部感知</td>
<td>采用卷积操作实现图像特征提取，具有“局部感知”力。</td>
<td>模仿生物的视知觉机制，通过局部感知的方式处理图像。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>参数共享</td>
<td>指某个特征图中的所有神经元使用相同的“权值和偏置”。</td>
<td>l可大大减少模型的参数数量和运算时间，提升效率。  l实现了对不同位置上的局部特征的相同处理。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>下采样  （池化）</td>
<td>指在特征映射上定期地探索每个子区域，并简化映射内容，将多个相邻像素的值合并成一个值。</td>
<td>可以逐渐降低数据体的空间尺寸，减少网络中参数的数量，使计算资源耗费变少，也能有效控制过拟合。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>层次化  表达</td>
<td>所提取的特征逐渐由高层次到低层次。（低层提取简单特  征，中间层变得抽象，高层次则更加抽象）</td>
<td>这种层次化表达使得CNN能从原始输入中提取出从“局部  到全局”的丰富特征。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>稀疏连接</td>
<td>与传统神经网络的全连接不同，CNN采用稀疏连接方式。</td>
<td>l这意味着图像中感兴趣特征可能只存在于图像上的一小块，而非整个图像。  l通过使用较小的卷积核，卷积神经网络能够有效地提取出这些特征，同时减少计算量和冗余。</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/image-20240718154152317.png" alt="image-20240718154152317"></p>
<h2 id="卷积神经网络CNN-应用"><a href="#卷积神经网络CNN-应用" class="headerlink" title="卷积神经网络CNN - 应用"></a>卷积神经网络CNN - 应用</h2><p><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/ai%E5%AF%BC%E8%AE%BA/b401369e053b4cc89b113959ef7c4854.png" alt="在这里插入图片描述"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://kashima19960.github.io">木人舟</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://kashima19960.github.io/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">https://kashima19960.github.io/2024/07/18/人工智能/9.神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kashima19960.github.io" target="_blank">木人舟的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/kashima19960/img@master/%E5%85%B6%E4%BB%96/86.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/30/mysql+php+html%E5%AE%9E%E7%8E%B0%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/" title="mysql+php+html实现学生管理系统"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">mysql+php+html实现学生管理系统</div></div></a></div><div class="next-post pull-right"><a href="/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/8.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/" title="8.机器学习概述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">8.机器学习概述</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/1.AI%E7%BB%AA%E8%AE%BA%E4%B8%8E%E6%A6%82%E8%BF%B0/" title="1.绪论与概述"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">1.绪论与概述</div></div></a></div><div><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/2.1%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA/" title="2.1知识表示"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">2.1知识表示</div></div></a></div><div><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/2.2%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/" title="2.2知识图谱"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">2.2知识图谱</div></div></a></div><div><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/3.%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8E%A8%E7%90%86/" title="3.确定性推理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">3.确定性推理</div></div></a></div><div><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/4.%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E6%8E%A8%E7%90%86%E6%96%B9%E6%B3%95/" title="4.不确定性推理方法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">4.不确定性推理方法</div></div></a></div><div><a href="/2024/07/17/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/5.%E6%90%9C%E7%B4%A2%E6%B1%82%E8%A7%A3%E7%AD%96%E7%95%A5/" title="5搜索求解策略"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">5搜索求解策略</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/kashima19960/img@master/%E5%85%B6%E4%BB%96/86.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">木人舟</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">53</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kashima19960"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kashima19960" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="/CodingCV@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">阅读文章遇到问题可以发消息到我的邮箱——CodingCV@outlook.com</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%96%87%E7%AB%A0%E7%B3%BB%E5%88%97"><span class="toc-number">1.</span> <span class="toc-text">人工智能文章系列</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">2.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E8%84%91%E7%BB%93%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text">人脑结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E8%84%91%E6%9E%84%E9%80%A0"><span class="toc-number">3.1.</span> <span class="toc-text">人脑构造</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E8%84%91%E7%A5%9E%E7%BB%8F%E5%85%83%E7%BB%93%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">人脑神经元结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E8%84%91%E7%A5%9E%E7%BB%8F%E5%85%83%E7%BB%93%E6%9E%84-%E2%80%93-%E6%9C%AF%E8%AF%AD"><span class="toc-number">5.</span> <span class="toc-text">人脑神经元结构 – 术语</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%B5%B7%E6%BA%90%E4%B8%8E%E6%A6%82%E5%BF%B5"><span class="toc-number">6.</span> <span class="toc-text">人工神经网络 - 起源与概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E6%BC%94%E8%BF%9B-%E4%B8%89%E8%B5%B7%E4%B8%89%E8%90%BD"><span class="toc-number">7.</span> <span class="toc-text">神经网络发展演进 - 三起三落</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0-%E5%86%B3%E5%AE%9A%E6%80%A7%E8%83%BD%E7%9A%84%E4%B8%89%E5%A4%A7%E8%A6%81%E7%B4%A0"><span class="toc-number">8.</span> <span class="toc-text">神经网络的学习 - 决定性能的三大要素</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E5%AE%9A%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%A7%E8%83%BD%E7%9A%84%E4%B8%89%E5%A4%A7%E8%A6%81%E7%B4%A0"><span class="toc-number">8.1.</span> <span class="toc-text">决定人工神经网络性能的三大要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hebb%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%99"><span class="toc-number">8.2.</span> <span class="toc-text">Hebb学习规则</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-%E5%88%86%E7%B1%BB"><span class="toc-number">9.</span> <span class="toc-text">神经网络结构 - 分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E5%9E%8B%EF%BC%88-%E5%89%8D%E5%90%91%E5%9E%8B%EF%BC%89"><span class="toc-number">9.1.</span> <span class="toc-text">前馈型（ 前向型）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E9%A6%88%E5%9E%8B"><span class="toc-number">9.2.</span> <span class="toc-text">反馈型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-%E5%BA%94%E7%94%A8"><span class="toc-number">10.</span> <span class="toc-text">神经网络结构 - 应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83-MP%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%89"><span class="toc-number">11.</span> <span class="toc-text">神经元 - MP模型（单层感知机）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9C%9F%E5%AE%9E%E7%9A%84%E4%BA%BA%E8%84%91%E7%A5%9E%E7%BB%8F3D%E6%A8%A1%E6%8B%9F"><span class="toc-number">11.1.</span> <span class="toc-text">真实的人脑神经3D模拟</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B"><span class="toc-number">12.</span> <span class="toc-text">单层感知机 - 数学模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88%E8%BE%93%E5%87%BA%E5%8F%98%E6%8D%A2%E5%87%BD%E6%95%B0%EF%BC%89"><span class="toc-number">13.</span> <span class="toc-text">单层感知机 - 激活函数（输出变换函数）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E6%9C%AC%E8%B4%A8-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">14.</span> <span class="toc-text">单层感知机的本质 - 线性分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E5%B1%80%E9%99%90%E6%80%A7%EF%BC%88%E6%97%A0%E6%B3%95%E8%AE%A1%E7%AE%97%E5%BC%82%E6%88%96XOR%EF%BC%89"><span class="toc-number">15.</span> <span class="toc-text">单层感知机 - 局限性（无法计算异或XOR）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%98%8E%E6%96%AF%E5%9F%BA%E7%9A%84%E2%80%9C%E5%8A%9F%E4%B8%8E%E7%BD%AA%E2%80%9D"><span class="toc-number">16.</span> <span class="toc-text">两层感知机 - 明斯基的“功与罪”</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E7%BD%97%E6%A3%AE%E5%B8%83%E6%8B%89%E7%89%B9%EF%BC%880%E5%88%B01%E7%9A%84%E7%AA%81%E7%A0%B4%EF%BC%89"><span class="toc-number">17.</span> <span class="toc-text">两层感知机 - 罗森布拉特（0到1的突破）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8-%E8%A7%A3%E5%86%B3%E5%BC%82%E6%88%96XOR%E9%97%AE%E9%A2%98"><span class="toc-number">18.</span> <span class="toc-text">两层感知器 - 解决异或XOR问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8-%E8%A7%A3%E5%86%B3%E5%BC%82%E6%88%96XOR%E9%97%AE%E9%A2%98-1"><span class="toc-number">19.</span> <span class="toc-text">两层感知器 - 解决异或XOR问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E7%9A%84%E6%9C%AC%E8%B4%A8-%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">20.</span> <span class="toc-text">两层感知器的本质 - 非线性分类器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%881%E5%88%B0100%E7%9A%84%E5%8F%91%E5%B1%95%EF%BC%89"><span class="toc-number">21.</span> <span class="toc-text">多层感知机 - 深度学习（1到100的发展）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E5%B0%8F%E7%BB%93-%E7%AE%97%E6%B3%95%E3%80%81%E7%AE%97%E5%8A%9B%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">22.</span> <span class="toc-text">神经网络发展小结 - 算法、算力的影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E5%B0%8F%E7%BB%93-%E6%89%A9%E5%B1%95"><span class="toc-number">23.</span> <span class="toc-text">神经网络发展小结 - 扩展</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hopfield%E7%BD%91%E7%BB%9C-%E6%A6%82%E5%BF%B5%E4%B8%8E%E5%8E%9F%E7%90%86"><span class="toc-number">24.</span> <span class="toc-text">Hopfield网络 - 概念与原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hopfield%E7%BD%91%E7%BB%9C-%E4%BC%8A%E8%BE%9B%E6%A8%A1%E5%9E%8B%EF%BC%88Ising-model%EF%BC%89"><span class="toc-number">25.</span> <span class="toc-text">Hopfield网络 - 伊辛模型（Ising model）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hopfield%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-number">26.</span> <span class="toc-text">Hopfield神经网络 - 网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hopfield%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%B9%B3%E8%A1%A1%E7%8A%B6%E6%80%81%E8%A7%84%E5%88%99"><span class="toc-number">27.</span> <span class="toc-text">Hopfield神经网络 - 平衡状态规则</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97"><span class="toc-number">27.1.</span> <span class="toc-text">权重计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E8%AE%A1%E7%AE%97"><span class="toc-number">27.2.</span> <span class="toc-text">结果计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hopfield%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%B1%82%E8%A7%A3%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5"><span class="toc-number">28.</span> <span class="toc-text">Hopfield神经网络 - 求解权重矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hopfield%E7%BD%91%E7%BB%9C-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E7%89%87%E6%AE%B5%E5%9B%9E%E5%BF%86%E6%95%B4%E4%B8%AA%E8%AE%B0%E5%BF%86%E5%86%85%E5%AE%B9"><span class="toc-number">29.</span> <span class="toc-text">Hopfield网络 - 如何通过片段回忆整个记忆内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A6%82%E5%BF%B5"><span class="toc-number">30.</span> <span class="toc-text">BP神经网络 - 概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E8%B5%B7%E6%BA%90"><span class="toc-number">31.</span> <span class="toc-text">BP神经网络 - 起源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">32.</span> <span class="toc-text">BP神经网络 - 算法原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%EF%BC%88%E9%80%9A%E4%BF%97%E4%B8%BE%E4%BE%8B%E8%A7%A3%E9%87%8A%EF%BC%89"><span class="toc-number">33.</span> <span class="toc-text">BP神经网络 - 算法原理（通俗举例解释）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B"><span class="toc-number">34.</span> <span class="toc-text">BP神经网络 - 数学模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">35.</span> <span class="toc-text">BP神经网络 - 前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%E5%8F%8D%E9%A6%88%E4%B8%8E%E8%B0%83%E6%95%B4%EF%BC%89"><span class="toc-number">36.</span> <span class="toc-text">BP神经网络 - 反向传播（反馈与调整）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E5%B0%8F%E7%BB%93"><span class="toc-number">37.</span> <span class="toc-text">BP神经网络 - 小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN-%E8%B5%B7%E6%BA%90%E4%B8%8E%E5%8E%9F%E7%90%86"><span class="toc-number">38.</span> <span class="toc-text">卷积神经网络CNN - 起源与原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN-%E7%AA%81%E7%A0%B4-LeNet-5"><span class="toc-number">39.</span> <span class="toc-text">卷积神经网络CNN - 突破(LeNet-5)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E7%BB%93%E6%9E%84"><span class="toc-number">40.</span> <span class="toc-text">卷积神经网络 - 结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B61-%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%88%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%EF%BC%89"><span class="toc-number">41.</span> <span class="toc-text">CNN算法框架1 - 卷积层（卷积操作）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%88%E5%AD%90%E5%8D%B7%E7%A7%AF%E5%B1%82%EF%BC%89"><span class="toc-number">42.</span> <span class="toc-text">CNN算法框架 - 卷积层（子卷积层）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">43.</span> <span class="toc-text">CNN算法框架 - 卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-number">43.1.</span> <span class="toc-text">填充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E5%8D%B7%E7%A7%AF%E7%9A%84%E6%AD%A5%E9%95%BF"><span class="toc-number">43.2.</span> <span class="toc-text">选择卷积的步长</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%A0%B8%E4%B8%AA%E6%95%B0%E7%9A%84%E7%A1%AE%E5%AE%9A"><span class="toc-number">43.3.</span> <span class="toc-text">卷积核个数的确定</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88%E4%B8%8B%E9%87%87%E6%A0%B7%EF%BC%89"><span class="toc-number">44.</span> <span class="toc-text">CNN算法框架 - 池化层（下采样）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90%EF%BC%89"><span class="toc-number">45.</span> <span class="toc-text">CNN算法框架 - 池化层（技术分析）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">46.</span> <span class="toc-text">CNN算法框架 - 激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E5%88%86%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Batch-Nomarliaztion%EF%BC%89"><span class="toc-number">47.</span> <span class="toc-text">CNN算法框架 - 分批归一化（Batch Nomarliaztion）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82-dense-layer"><span class="toc-number">48.</span> <span class="toc-text">CNN算法框架 - 全连接层 (dense layer)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN-%E7%89%B9%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-number">49.</span> <span class="toc-text">卷积神经网络CNN - 特点总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN-%E5%BA%94%E7%94%A8"><span class="toc-number">50.</span> <span class="toc-text">卷积神经网络CNN - 应用</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/11/scoop%E9%80%80%E5%9B%9E%E8%BD%AF%E4%BB%B6%E7%89%88%E6%9C%AC%E7%9A%84%E6%96%B9%E6%B3%95/" title="scoop退回软件版本的方法">scoop退回软件版本的方法</a><time datetime="2025-03-11T15:53:00.000Z" title="发表于 2025-03-11 23:53:00">2025-03-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/10/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" title="正则表达式快速入门">正则表达式快速入门</a><time datetime="2025-03-10T15:58:20.000Z" title="发表于 2025-03-10 23:58:20">2025-03-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/10/%E5%9E%83%E5%9C%BE%E4%BB%A3%E7%A0%81%E7%BC%96%E5%86%99%E6%8C%87%E5%8D%97_C%E8%AF%AD%E8%A8%80%E7%89%88/" title="垃圾代码编写指南_C语言版">垃圾代码编写指南_C语言版</a><time datetime="2025-03-10T09:23:00.000Z" title="发表于 2025-03-10 17:23:00">2025-03-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/23/Linux/%E7%A1%AC%E9%93%BE%E6%8E%A5VS%E8%BD%AF%E9%93%BE%E6%8E%A5/" title="Linux软连接与硬链接">Linux软连接与硬链接</a><time datetime="2025-02-23T13:54:00.000Z" title="发表于 2025-02-23 21:54:00">2025-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/14/%E5%BE%AE%E6%9C%BA/%E7%AE%80%E7%AD%94%E9%A2%98%E9%A2%98%E5%BA%93%20(%E7%AD%94%E6%A1%88)/" title="《微型计算机技术与应用》期末考试题库简答题答案">《微型计算机技术与应用》期末考试题库简答题答案</a><time datetime="2025-01-14T05:05:00.000Z" title="发表于 2025-01-14 13:05:00">2025-01-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 木人舟</div><div class="footer_custom_text">木人舟 ALL RIGHTS RESERVED.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !true) {
    if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'https://kashima19960.github.io/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/'
    this.page.identifier = '/2024/07/18/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/9.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/'
    this.page.title = '9.神经网络'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !true) {
    if (true) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>